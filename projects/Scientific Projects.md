Here is a running list of project I'm working on or have worked on. Click through to read more. 

# Input-Driven Neural Stability

What's the saying again? A butterfly flaps its wings in Kyiv, Ukraine and 30 years later there's a storm in Cambridge, MA? Complex systems--like the weather--are prone to instability and chaos. But the most complex system we know, this three-pound gelatinous blob in our heads, is incredibly stable. You have an animal do a task over and over while recording from its brain and you'll find that the brain does more-or-less something similar each time. You can adjust the way you hold a cup of coffee on the fly to keep it from spilling over. From the perspective of dynamical systems theory, this is amazing! You don't just get properties like that by accident.  Keep in mind the brain has to stable even when its getting bombarded by inputs from the outside world and synaptic noise from within. Most mathematical theories of neural stability--heck, stability in general--focus on the stability of *autonomous* systems. That is, systems evolving without any input from the outside. This is obviously not appropriate for understanding stability in brains. 

## Achieving Input-Driven Stability in Artificial Neural Networks

To start getting a hold on these questions, we applied contraction analysis to artificial recurrent neural networks (RNNs).

## Robust Working Memory Through Stabilizing Plasticity

One theory for working memory is that sensory inputs induce structural changes in neural circuits. 

# Stability Theory for Deep Learning

## Recursive Combinations of Stable Recurrent Neural Networks 

## How Do Unstable Networks Operate in the Stable Regime? 

# Glial Computation 

I've been interested in glial computation since 2016, when I was a research assistant at COMBRA Lab at Rutgers University. These cells make up the majority of the brain, they are *extremely* active and dynamic and interesting, yet they are almost universally ignored by computational neuroscientists! While I was at COMBRA we proposed two potential roles for glial (specifically, astrocyte) computation. 

The first was the simple observation that given the observed geometry of astrocytes in cortex and observed wave-front speeds of calcium waves in astrocytic networks, slow-wave neural oscillations could be coordinated by astrocytes.

The second observation was slightly more technical. We noticed that if you simplified the equations for the tripartite synapse as much as you could and assumed a type of Hebbian-learning, then you could get astrocyte-neural networks to have content-addressable *sequence memory*. For example you could show these networks a corrupted first frame of a short movie and they would play the rest of the movie for you. 

I plan on returning to this question at some point in the future, and leveraging e.g the power of deep learning to get some answers. 

## Slow  Wave Rhythmogenesis 

## Sequence Memory in Neural-Astrocyte Networks



